# Progress Log

## Learnings
(Notes discovered during implementation)

## Decisions
- PRD rewritten from scratch with expanded scope: price + sentiment + fundamentals data sources
- Story IDs reset to US-001..US-703 (no phase prefix in Phase 0, phase-prefixed from Phase 1 onward)
- ML retraining is core to the system (Phase 5), not optional

## Integration Notes
(Cross-story dependencies and gotchas)

## Next Smallest Task
- US-703: Add minimal HTML dashboard

---

## Iteration 1 — US-001: Create project skeleton
- What was done:
  - Created tests/conftest.py (was missing from scaffold)
  - Fixed mypy type error in migrations.py: `callable` → `Callable[[sqlite3.Connection], None]`
  - Added py.typed marker to src/smaps/ for mypy PEP 561 compliance
  - Added [tool.mypy] config to pyproject.toml (mypy_path, explicit_package_bases)
  - Verified: pip install -e . succeeds, mypy passes, all 9 tests pass
- Files affected: tests/conftest.py, src/smaps/migrations.py, src/smaps/py.typed, pyproject.toml, PRD.md
- Learnings for next cycles:
  - src layout + editable install causes mypy "found twice" error; fix with mypy_path = "src" + explicit_package_bases = true
  - Phase 0 scaffold was already largely built but PRD checkboxes were never ticked after rewrite
  - Use `Callable` from typing, not builtin `callable`, as a type annotation
---

## Iteration 2 — US-002: Add Makefile with test and lint targets
- What was done:
  - Added `lint` target (`ruff check src/ tests/`) and `typecheck` target (`mypy src/`) to Makefile
  - Added `ruff>=0.4` and `mypy>=1.0` to `[project.optional-dependencies] dev` in pyproject.toml
  - Removed stale pre-rewrite files from src/ root (learner.py, predictor.py, scheduler.py, validator.py, __init__.py) that were causing mypy failures — these were from the old project structure before the PRD rewrite
  - Verified: `make test` (9 passed), `make lint` (all checks passed), `make typecheck` (success) all exit 0
- Files affected: Makefile, pyproject.toml, PRD.md, src/__init__.py (deleted), src/learner.py (deleted), src/predictor.py (deleted), src/scheduler.py (deleted), src/validator.py (deleted)
- Learnings for next cycles:
  - Stale files in src/ root from pre-rewrite era cause mypy to fail; they were safely removed since the package lives at src/smaps/
  - ruff is a fast Python linter with zero config needed for basic linting
  - Makefile .PHONY list must include all non-file targets
---

## Iteration 3 — US-003: Add config loader with environment variable support
- What was done:
  - .env.example already existed with SMAPS_TICKERS, SMAPS_DB_PATH, SMAPS_LOG_LEVEL
  - src/smaps/config.py already existed with Settings class using pydantic-settings BaseSettings
  - test_config_defaults already existed verifying Settings() works with no env vars
  - Added test_config_env_overrides to verify SMAPS_* env var overrides work (monkeypatch)
  - Verified: make test (10 passed), make typecheck (success)
- Files affected: tests/test_smoke.py, PRD.md, progress.txt
- Learnings for next cycles:
  - Many Phase 0 stories were pre-built during scaffold but lacked tests for all acceptance criteria
  - pydantic-settings BaseSettings with env_prefix handles env var parsing including JSON lists
  - monkeypatch.setenv is the clean way to test env var overrides in pytest
---

## Iteration 4 — US-004: Add structured logging with run_id
- What was done:
  - src/smaps/logging.py already existed with get_logger(name, run_id=None)
  - Format already included ISO timestamp (%Y-%m-%dT%H:%M:%S), level, name, and run_id
  - Existing test_get_logger verified name but not isinstance; added isinstance(logger, logging.Logger) checks
  - Added test_get_logger_output_format that captures stderr and verifies ISO timestamp, level, name, run_id, and message are all present in output
  - Verified: make test (11 passed), make typecheck (success)
- Files affected: tests/test_smoke.py, PRD.md, progress.txt
- Learnings for next cycles:
  - logging.StreamHandler writes to stderr by default; use capfd to capture it
  - Use unique logger names in tests to avoid handler reuse from Python's global logger registry
  - Phase 0 modules were pre-built but needed AC-level test coverage to be marked complete
---

## Iteration 5 — US-005: Add SQLite database layer with schema migrations
- What was done:
  - db.py already exported get_connection(db_path) and ensure_schema(conn)
  - migrations.py already had sequential migration list with Callable types
  - schema_migrations table already created with version tracking
  - ensure_schema already idempotent (CREATE TABLE IF NOT EXISTS + version check)
  - Existing tests verified table creation and idempotency
  - Added test_schema_migrations_tracks_version to explicitly verify:
    - schema_migrations table exists after ensure_schema
    - Tracked version matches SCHEMA_VERSION
    - Only one version row exists (not accumulated from repeated calls)
  - Verified: make test (12 passed), make typecheck (success)
- Files affected: tests/test_smoke.py, PRD.md, progress.txt
- Learnings for next cycles:
  - db.py uses DELETE + INSERT pattern for set_schema_version (single-row tracking, not append)
  - get_schema_version returns 0 if schema_migrations table doesn't exist yet
  - ensure_schema has downgrade protection: raises RuntimeError if DB version > code version
  - Phase 0 DB layer was fully pre-built; just needed explicit test for version tracking AC
---

## Iteration 6 — US-101: Define OHLCV data model
- What was done:
  - Created src/smaps/models.py with OHLCVBar frozen dataclass (slots=True)
  - Fields: ticker (str), date (datetime.date), open/high/low/close (float), volume (int)
  - __post_init__ validation: high >= low, volume >= 0
  - Created tests/test_models.py with 5 tests:
    - test_create_valid_bar: verifies all fields
    - test_create_bar_high_equals_low: edge case (flat day)
    - test_validation_high_less_than_low: rejects invalid high/low
    - test_validation_negative_volume: rejects negative volume
    - test_bar_is_frozen: confirms immutability
  - Verified: make test (17 passed), make typecheck (success)
- Files affected: src/smaps/models.py (new), tests/test_models.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - frozen=True + slots=True on dataclass gives immutability + memory efficiency
  - __post_init__ works with frozen dataclasses for validation (runs before freeze)
  - volume is int (not float) since it represents share count
  - First Phase 1 file — models.py will be imported by collectors and features later
---

## Iteration 7 — US-102: Implement Yahoo Finance daily downloader
- What was done:
  - Created src/smaps/collectors/ package with __init__.py
  - Created src/smaps/collectors/price.py with fetch_daily_bars(ticker, start, end) -> list[OHLCVBar]
  - Uses yfinance.download() with auto_adjust=True, progress=False
  - End date offset by +1 day since yfinance end is exclusive
  - Created tests/test_collectors_price.py with 5 mocked tests:
    - test_returns_ohlcv_bars: verifies list[OHLCVBar] return type
    - test_parses_fields_correctly: verifies field mapping from DataFrame
    - test_empty_response: handles empty DataFrame gracefully
    - test_passes_auto_adjust: verifies auto_adjust=True kwarg
    - test_ticker_propagated: verifies ticker field set on all bars
  - Added yfinance>=0.2 to pyproject.toml dependencies
  - Added [[tool.mypy.overrides]] for yfinance.* to ignore_missing_imports (no py.typed)
  - Verified: make test (22 passed), make typecheck (success)
- Files affected: src/smaps/collectors/__init__.py (new), src/smaps/collectors/price.py (new), tests/test_collectors_price.py (new), pyproject.toml, PRD.md, progress.txt
- Learnings for next cycles:
  - yfinance has no py.typed marker; need mypy overrides with ignore_missing_imports = true
  - mypy cache can become stale after pyproject.toml config changes; clear .mypy_cache if overrides don't take effect
  - yfinance.download() end date is exclusive; add +1 day timedelta for inclusive end
  - Use @patch("smaps.collectors.price.yf.download") to mock at the module level where it's imported
  - DataFrame.iterrows() yields (Timestamp, Series) tuples; call .date() on timestamp for datetime.date
---

## Iteration 8 — US-103: Add OHLCV table and idempotent upsert
- What was done:
  - Updated migration_001 in migrations.py: removed `adj_close` column (not in OHLCVBar model since auto_adjust=True), changed volume from REAL to INTEGER to match OHLCVBar.volume (int)
  - Added `upsert_bars(conn, bars) -> int` function to db.py using INSERT OR REPLACE
  - Stores date as ISO text (YYYY-MM-DD) for SQLite compatibility
  - Created tests/test_ohlcv_persistence.py with 4 tests:
    - test_upsert_inserts_bar: verifies single bar round-trip
    - test_upsert_same_row_twice_yields_single_row: core AC — same PK twice → 1 row, second value wins
    - test_upsert_multiple_bars: verifies batch insert of 3 bars
    - test_upsert_empty_list: edge case — empty list succeeds
  - Fixed existing column-check tests in test_smoke.py and test_migrations.py to remove adj_close
  - Verified: make test (26 passed), make typecheck (success)
- Files affected: src/smaps/migrations.py, src/smaps/db.py, tests/test_ohlcv_persistence.py (new), tests/test_smoke.py, tests/test_migrations.py, PRD.md, progress.txt
- Learnings for next cycles:
  - Original scaffold migration_001 had adj_close from pre-PRD era; safe to modify since no production DB exists
  - INSERT OR REPLACE on composite PK (ticker, date) is the idiomatic SQLite upsert for this pattern
  - Store datetime.date as ISO string via .isoformat() — SQLite has no native date type
  - executemany() is efficient for batch inserts; commit once after the batch
---

## Iteration 9 — US-104: Implement sentiment data collector
- What was done:
  - Added SentimentScore frozen dataclass to src/smaps/models.py with validation (score clamped to -1..1)
  - Created src/smaps/collectors/sentiment.py with fetch_sentiment(ticker, date) -> SentimentScore
  - Uses Google News RSS feed as free data source (no API key needed)
  - Keyword-based headline sentiment heuristic: positive/negative word sets, averaged across headlines
  - _score_headline() scores individual headlines; _fetch_rss() fetches RSS XML
  - Created tests/test_collectors_sentiment.py with 10 tests:
    - TestScoreHeadline: positive, negative, neutral, mixed headlines, score range
    - TestFetchSentiment: returns SentimentScore, positive/negative/empty feed, valid range
  - All tests use mocked urlopen (no live HTTP calls)
  - Verified: make test (36 passed), make typecheck (success)
- Files affected: src/smaps/models.py, src/smaps/collectors/sentiment.py (new), tests/test_collectors_sentiment.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - Google News RSS is a free, no-auth-needed source for stock headlines: news.google.com/rss/search?q=TICKER+stock
  - urlopen returns a context manager; mock both __enter__/__exit__ and .read() for testing
  - Keyword-based sentiment is a reasonable baseline; can be upgraded to NLP later
  - SentimentScore.source field enables multiple providers in future (e.g., Reddit, Twitter)
  - xml.etree.ElementTree.fromstring() + .iter("title") is the simplest way to extract RSS titles
---

## Iteration 10 — US-105: Add sentiment table and persistence
- What was done:
  - Added migration_002_sentiment to migrations.py creating `sentiment_daily` table with PK (ticker, date, source)
  - Bumped SCHEMA_VERSION from 1 to 2 in db.py
  - Added `upsert_sentiment(conn, scores) -> int` function to db.py using INSERT OR REPLACE
  - Imported SentimentScore into db.py alongside OHLCVBar
  - Created tests/test_sentiment_persistence.py with 5 tests:
    - test_upsert_inserts_sentiment: verifies single score round-trip
    - test_upsert_same_row_twice_yields_single_row: core AC — same PK twice → 1 row, second value wins
    - test_upsert_multiple_scores: verifies batch insert of 3 scores
    - test_upsert_different_sources_same_ticker_date: verifies different sources coexist (composite PK)
    - test_upsert_empty_list: edge case — empty list succeeds
  - Verified: make test (41 passed), make typecheck (success)
- Files affected: src/smaps/migrations.py, src/smaps/db.py, tests/test_sentiment_persistence.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - SCHEMA_VERSION bump is safe because existing tests reference it dynamically via import
  - Composite PK (ticker, date, source) allows multiple sentiment providers for the same ticker+date
  - INSERT OR REPLACE on composite PK is the same idempotent upsert pattern used for ohlcv_daily
  - Migration pattern is consistent: add function, add to MIGRATIONS list, bump SCHEMA_VERSION
---

## Iteration 11 — US-106: Implement fundamentals data collector
- What was done:
  - Added Fundamentals frozen dataclass to src/smaps/models.py with fields: ticker, date, pe_ratio, market_cap, eps, revenue (all optional floats except ticker/date)
  - Created src/smaps/collectors/fundamentals.py with fetch_fundamentals(ticker) -> Fundamentals
  - Uses yfinance Ticker(ticker).info to retrieve trailingPE, marketCap, trailingEps, totalRevenue
  - Helper _float_or_none() safely converts values, returning None for missing/non-numeric data
  - Created tests/test_collectors_fundamentals.py with 6 mocked tests:
    - test_returns_fundamentals: verifies Fundamentals return type
    - test_parses_all_fields: verifies field mapping from info dict
    - test_partial_info_returns_none_for_missing: missing keys → None
    - test_empty_info_all_none: no fundamental keys → all None
    - test_ticker_passed_to_yfinance: verifies yf.Ticker() called with correct ticker
    - test_non_numeric_value_returns_none: "N/A" and non-numeric strings → None
  - Verified: make test (47 passed), make typecheck (success)
- Files affected: src/smaps/models.py, src/smaps/collectors/fundamentals.py (new), tests/test_collectors_fundamentals.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - yfinance Ticker.info returns a dict[str, Any]; key names are camelCase (trailingPE, marketCap, trailingEps, totalRevenue)
  - Info dict may contain "N/A" strings or None for missing metrics; always guard with try/except on float()
  - Fundamentals uses date=datetime.date.today() since yfinance .info returns current snapshot, not historical
  - Mock yf.Ticker at module level: @patch("smaps.collectors.fundamentals.yf.Ticker")
  - Fundamentals dataclass uses Optional[float] (float | None) for all metric fields since any can be unavailable
---

## Iteration 12 — US-107: Add fundamentals table and persistence
- What was done:
  - Added migration_003_fundamentals to migrations.py creating `fundamentals_daily` table with PK (ticker, date)
  - Columns: ticker (TEXT), date (TEXT), pe_ratio (REAL), market_cap (REAL), eps (REAL), revenue (REAL)
  - Bumped SCHEMA_VERSION from 2 to 3 in db.py
  - Added `upsert_fundamentals(conn, rows) -> int` function to db.py using INSERT OR REPLACE
  - Imported Fundamentals into db.py alongside OHLCVBar and SentimentScore
  - Created tests/test_fundamentals_persistence.py with 5 tests:
    - test_upsert_inserts_fundamentals: verifies single row round-trip with all fields
    - test_upsert_same_row_twice_yields_single_row: core AC — same PK twice → 1 row, second value wins
    - test_upsert_multiple_fundamentals: verifies batch insert of 3 rows
    - test_upsert_with_none_fields: verifies None/Optional fields stored as NULL
    - test_upsert_empty_list: edge case — empty list succeeds
  - Verified: make test (52 passed), make typecheck (success)
- Files affected: src/smaps/migrations.py, src/smaps/db.py, tests/test_fundamentals_persistence.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - Fundamentals PK is (ticker, date) — simpler than sentiment which has (ticker, date, source) composite
  - All metric columns are nullable REAL since Fundamentals fields are Optional[float]
  - None values in Python pass through to SQLite as NULL via executemany without special handling
  - Migration pattern remains consistent: add function, add to MIGRATIONS list, bump SCHEMA_VERSION
  - Phase 1 data collection is now complete (US-101 through US-107); US-108 orchestrator is next
---

## Iteration 13 — US-108: Add multi-ticker ingestion orchestrator
- What was done:
  - Created src/smaps/collectors/ingest.py with `ingest_all(tickers, start, end, db_path)` function
  - Calls fetch_daily_bars, fetch_sentiment, fetch_fundamentals for each ticker
  - Error in one ticker is caught and logged; does not block other tickers
  - Returns dict with "succeeded" and "failed" ticker lists for caller visibility
  - Each collector step logged with timing via time.monotonic()
  - Private helper `_ingest_ticker()` handles single-ticker orchestration
  - Created tests/test_ingest.py with 5 tests:
    - test_ingest_all_succeeds_for_multiple_tickers: all tickers succeed
    - test_error_in_one_ticker_does_not_block_others: failing ticker isolated
    - test_calls_all_three_collectors_per_ticker: verifies call args
    - test_empty_ticker_list: edge case — no tickers
    - test_data_persisted_to_database: end-to-end flow completes
  - All external calls mocked (no live API calls in tests)
  - Verified: make test (57 passed), make typecheck (success)
- Files affected: src/smaps/collectors/ingest.py (new), tests/test_ingest.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - @patch decorators stack in reverse order as function parameters (outermost decorator = last param)
  - time.monotonic() is the correct timer for elapsed-time measurement (not affected by system clock changes)
  - Return value dict pattern (succeeded/failed) makes orchestrator testable without DB inspection
  - Phase 1 data collection is now fully complete (US-101 through US-108); Phase 2 feature engineering is next
---

## Iteration 14 — US-201: Define feature pipeline interface
- What was done:
  - Created src/smaps/features/ package with __init__.py
  - Created src/smaps/features/pipeline.py with FeaturePipeline Protocol class
  - Protocol defines `transform(ticker, as_of_date) -> dict[str, float]` method
  - Docstring specifies no-future-data contract: only data dated <= as_of_date may be used
  - Created tests/test_feature_pipeline.py with 3 tests:
    - test_dummy_implements_protocol: concrete class satisfies FeaturePipeline type annotation
    - test_transform_returns_dict_of_floats: verifies return type contract
    - test_protocol_is_runtime_checkable_via_typing: verifies structural subtyping via hasattr
  - Verified: make test (60 passed), make typecheck (success)
- Files affected: src/smaps/features/__init__.py (new), src/smaps/features/pipeline.py (new), tests/test_feature_pipeline.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - typing.Protocol provides structural subtyping — no explicit inheritance required from implementors
  - Protocol methods use `...` (Ellipsis) as body, not `pass`, per convention
  - FeaturePipeline is the base interface for US-202 (technical), US-203 (sentiment), US-204 (fundamental) features
  - First Phase 2 file — features/ package will be the home for all feature engineering code
  - Next stories (US-202..US-204) will create concrete implementations of this protocol
---

## Iteration 15 — US-202: Implement technical indicator features
- What was done:
  - Created src/smaps/features/technical.py with TechnicalFeatures class implementing FeaturePipeline protocol
  - Takes sqlite3.Connection in constructor; queries ohlcv_daily table filtered by date <= as_of_date
  - Computes 7 features: return_1d, return_5d, return_10d, ma_ratio_5_20, volume_change_1d, volatility_20d, rsi_14
  - All indicators return float('nan') when insufficient data is available
  - RSI uses simple average gain/loss method (SMA-based RSI)
  - No-leakage: SQL WHERE clause restricts to date <= as_of_date.isoformat()
  - Created tests/test_features_technical.py with 11 tests:
    - test_output_keys_and_shape: verifies all 7 keys returned as floats
    - test_return_1d: verifies 1-day return calculation
    - test_return_5d: verifies 5-day return calculation
    - test_return_10d: verifies 10-day return calculation
    - test_volume_change_1d: verifies volume change calculation
    - test_rsi_all_gains: RSI = 100 when all changes positive
    - test_rsi_all_losses: RSI = 0 when all changes negative
    - test_ma_ratio_5_20: verifies MA(5)/MA(20) ratio
    - test_no_leakage: future bars excluded by as_of_date filter
    - test_insufficient_data_returns_nan: 1 bar → all NaN
    - test_no_bars_returns_nan: unknown ticker → all NaN
  - Verified: make test (71 passed), make typecheck (success)
- Files affected: src/smaps/features/technical.py (new), tests/test_features_technical.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - float('nan') is a valid Python float; use math.isnan() to check in tests
  - SQL date comparison works on ISO strings (YYYY-MM-DD) in SQLite for <= filtering
  - TechnicalFeatures needs DB connection passed in constructor — same pattern will apply to SentimentFeatures and FundamentalFeatures
  - _return_n() is reusable for both price returns and volume change (same formula)
  - RSI edge cases: all gains → 100.0 (avg_loss=0 guard), all losses → 0.0 (natural from formula)
  - volatility_20d uses sample stdev (N-1 denominator) of daily returns over last 20 bars
---

## Iteration 16 — US-203: Implement sentiment features
- What was done:
  - Created src/smaps/features/sentiment.py with SentimentFeatures class implementing FeaturePipeline protocol
  - Takes sqlite3.Connection in constructor; queries sentiment_daily table filtered by date <= as_of_date
  - Computes 2 features: latest_sentiment_score, sentiment_ma_5d (5-day rolling average)
  - Gracefully returns 0.0 for both features when no sentiment data available
  - _rolling_avg() helper averages last N scores, or all available if fewer than N
  - No-leakage: SQL WHERE clause restricts to date <= as_of_date.isoformat()
  - Created tests/test_features_sentiment.py with 7 tests:
    - test_output_keys_and_types: verifies both keys returned as floats
    - test_latest_sentiment_score: verifies most recent score is returned
    - test_latest_sentiment_score_respects_as_of_date: future scores excluded
    - test_sentiment_ma_5d_with_enough_data: verifies 5-day rolling average
    - test_sentiment_ma_5d_with_fewer_than_5: averages all available when < 5
    - test_no_data_returns_zeros: both features return 0.0 for unknown ticker
    - test_no_data_for_ticker_but_other_tickers_exist: isolation between tickers
  - Verified: make test (78 passed), make typecheck (success)
- Files affected: src/smaps/features/sentiment.py (new), tests/test_features_sentiment.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - SentimentFeatures follows exact same pattern as TechnicalFeatures: conn in __init__, SQL query with date <= as_of_date
  - sentiment_daily table may have multiple sources per (ticker, date); SQL query returns all rows — this means scores list may include multiple scores per day
  - 0.0 is the graceful fallback for missing sentiment data (not NaN like technical features) since 0.0 represents neutral sentiment
  - _rolling_avg uses values[-window:] slicing which safely returns fewer items when len < window
  - US-204 (fundamental features) will follow the same pattern with fundamentals_daily table
---

## Iteration 17 — US-204: Implement fundamental features
- What was done:
  - Created src/smaps/features/fundamental.py with FundamentalFeatures class implementing FeaturePipeline protocol
  - Takes sqlite3.Connection in constructor; queries fundamentals_daily table filtered by date <= as_of_date
  - Computes 3 features: pe_ratio, eps, market_cap (latest available values via ORDER BY date DESC LIMIT 1)
  - Gracefully returns float('nan') for missing fields (None in DB → NaN) and when no data exists
  - _load_latest() helper fetches the single most recent row for the ticker
  - Created tests/test_features_fundamental.py with 8 tests:
    - test_output_keys_and_types: verifies all 3 keys returned as floats
    - test_pe_ratio_value: verifies pe_ratio from latest row
    - test_eps_value: verifies eps from latest row
    - test_market_cap_value: verifies market_cap from latest row
    - test_latest_row_used: multiple rows → picks most recent <= as_of_date
    - test_none_fields_return_nan: individual None fields → NaN
    - test_no_data_returns_all_nan: unknown ticker → all NaN
    - test_no_data_for_ticker_but_other_tickers_exist: isolation between tickers
  - Verified: make test (86 passed), make typecheck (success)
- Files affected: src/smaps/features/fundamental.py (new), tests/test_features_fundamental.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - FundamentalFeatures uses ORDER BY date DESC LIMIT 1 pattern (single latest row) vs SentimentFeatures which loads all rows for rolling computation
  - NaN (not 0.0) is the appropriate fallback for fundamentals since 0.0 would be a misleading value for pe_ratio, eps, market_cap
  - fundamentals_daily PK is (ticker, date) — no source dimension unlike sentiment
  - SQLite returns None for NULL columns; convert with `float(x) if x is not None else float("nan")`
  - US-205 (combine all features) is next — will merge TechnicalFeatures + SentimentFeatures + FundamentalFeatures
---

## Iteration 18 — US-205: Combine all features into unified vector
- What was done:
  - Created src/smaps/features/combined.py with `build_features(conn, ticker, as_of_date) -> dict[str, float]`
  - Merges TechnicalFeatures (7 keys) + SentimentFeatures (2 keys) + FundamentalFeatures (3 keys) = 12 total features
  - Exported FEATURE_KEYS frozenset for canonical key set reference
  - Key set is always consistent: technical/fundamental use NaN for missing data, sentiment uses 0.0
  - Created tests/test_features_combined.py with 8 tests:
    - test_returns_all_expected_keys: verifies exact FEATURE_KEYS match
    - test_all_values_are_floats: verifies all values are float type
    - test_consistent_keys_regardless_of_data: same keys with and without data
    - test_merges_technical_features: technical features present and finite with price data
    - test_merges_sentiment_features: sentiment features present with sentiment data
    - test_merges_fundamental_features: fundamental features present with fundamental data
    - test_no_data_defaults: NaN for technical/fundamental, 0.0 for sentiment when no data
    - test_full_data_all_keys_present: all 12 features finite when all data sources populated
  - Verified: make test (94 passed), make typecheck (success)
- Files affected: src/smaps/features/combined.py (new), tests/test_features_combined.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - build_features takes conn as first arg (same pattern as sub-pipelines needing DB access)
  - dict.update() is the simplest merge — no key conflicts since each pipeline has unique keys
  - FEATURE_KEYS frozenset is useful for downstream consumers (model training, snapshot validation)
  - 12 total features: 7 technical + 2 sentiment + 3 fundamental
  - US-206 (feature snapshot persistence) is next — will need FEATURE_KEYS for validation
---

## Iteration 19 — US-206: Add feature snapshot persistence
- What was done:
  - Added migration_004_feature_snapshots to migrations.py creating `feature_snapshots` table
  - Columns: id (INTEGER PRIMARY KEY AUTOINCREMENT), ticker (TEXT), feature_date (TEXT), features_json (TEXT), pipeline_version (TEXT)
  - Bumped SCHEMA_VERSION from 3 to 4 in db.py
  - Added FeatureSnapshot NamedTuple to db.py: id, ticker, feature_date, features, pipeline_version
  - Added `save_feature_snapshot(conn, ticker, feature_date, features, pipeline_version) -> int` returning row id
  - Added `load_feature_snapshot(conn, snapshot_id) -> FeatureSnapshot | None` for retrieval by id
  - Features stored as JSON text via json.dumps/json.loads
  - Created tests/test_feature_snapshot_persistence.py with 6 tests:
    - test_save_and_load_round_trip: core AC — save snapshot, load back, verify all fields equal
    - test_save_returns_autoincrement_id: each save gets unique auto-incremented id
    - test_load_nonexistent_returns_none: missing id returns None
    - test_features_with_nan_values: NaN values handled (JSON encodes as null)
    - test_multiple_snapshots_same_ticker_date: same ticker+date can have multiple snapshots (different pipeline versions)
    - test_feature_snapshot_named_tuple: verifies FeatureSnapshot structure
  - Verified: make test (100 passed), make typecheck (success)
- Files affected: src/smaps/migrations.py, src/smaps/db.py, tests/test_feature_snapshot_persistence.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - feature_snapshots uses AUTOINCREMENT id (not composite PK) since same ticker+date can have multiple snapshots
  - json.dumps/json.loads is the simplest way to serialize dict[str, float] for SQLite TEXT column
  - NaN values in JSON: Python json.dumps encodes float('nan') as NaN (non-standard JSON) — json.loads decodes back correctly in Python but may cause issues with other JSON parsers
  - NamedTuple is lighter than dataclass for simple value types that don't need validation
  - cursor.lastrowid gives the AUTOINCREMENT id after INSERT
  - US-207 (leakage prevention tests) is next — will verify no future data leaks into features
---

## Iteration 20 — US-207: Add leakage prevention tests
- What was done:
  - Created tests/test_leakage_prevention.py with 7 tests in 2 test classes
  - TestFutureBarExcluded (4 tests):
    - test_technical_excludes_future_bar: inject future OHLCV bar (close=999), verify features at T unchanged
    - test_sentiment_excludes_future_score: inject future sentiment (score=-0.9), verify features at T unchanged
    - test_fundamental_excludes_future_row: inject future fundamentals (pe_ratio=999), verify features at T unchanged
    - test_combined_features_exclude_all_future_data: inject future data across ALL sources, verify build_features at T unchanged
  - TestSnapshotDateConstraint (3 tests):
    - test_snapshot_feature_date_equals_as_of_date: saved snapshot feature_date == as_of_date
    - test_snapshot_feature_date_not_in_future: multiple snapshots all have feature_date <= as_of_date
    - test_snapshot_preserves_features_at_point_in_time: snapshot at T still matches original after future data added
  - Pattern: compute features before injecting future data, inject wildly different future data, recompute features at same as_of_date, assert identical results
  - Verified: make test (107 passed), make typecheck (success)
- Files affected: tests/test_leakage_prevention.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - Leakage prevention relies on SQL WHERE clause `date <= ?` in each pipeline's load method — simple but effective
  - Compare before/after approach is the most robust leakage test: no need to know internal implementation
  - NaN comparison requires special handling: math.isnan() check before pytest.approx()
  - Feature snapshots are immutable once saved — loading from DB always returns the original features regardless of later data changes
  - Phase 2 feature engineering is now fully complete (US-201 through US-207); Phase 3 (prediction model) is next
---

## Iteration 21 — US-301: Define prediction result model
- What was done:
  - Added Direction enum (UP/DOWN) to src/smaps/models.py
  - Added PredictionResult frozen dataclass to src/smaps/models.py with fields: ticker, prediction_date, direction (Direction enum), confidence (float 0-1), model_version (str)
  - __post_init__ validation: direction must be Direction enum, confidence must be 0.0..1.0
  - Added 8 tests to tests/test_models.py in TestPredictionResult class:
    - test_create_valid_prediction: verifies all fields
    - test_direction_down: verifies Direction.DOWN accepted
    - test_direction_enum_values: verifies exactly UP and DOWN members
    - test_confidence_boundary_zero: edge case confidence=0.0
    - test_confidence_boundary_one: edge case confidence=1.0
    - test_confidence_too_high: rejects confidence > 1.0
    - test_confidence_negative: rejects negative confidence
    - test_prediction_is_frozen: confirms immutability
  - Verified: make test (115 passed), make typecheck (success)
- Files affected: src/smaps/models.py, tests/test_models.py, PRD.md, progress.txt
- Learnings for next cycles:
  - Direction uses Enum (not Literal) for runtime type safety and .value access
  - PredictionResult follows same frozen+slots+__post_init__ pattern as OHLCVBar and SentimentScore
  - model_version is str (e.g., "v1") — will be used by model registry in US-303
  - First Phase 3 file — PredictionResult will be used by trainer (US-302), predictor (US-304), and persistence (US-305)
  - US-302 (train baseline model) is next — will use PredictionResult as the output type for predictions
---

## Iteration 22 — US-302: Train baseline model (Logistic Regression)
- What was done:
  - Created src/smaps/model/ package with __init__.py
  - Created src/smaps/model/trainer.py with TrainedModel dataclass and train_model() function
  - TrainedModel wraps sklearn Pipeline and provides predict(features) -> (Direction, confidence) method
  - train_model(features_df, labels, test_ratio) uses time-based split (no shuffle) — first 80% train, last 20% test
  - Pipeline: StandardScaler → LogisticRegression (random_state=42, max_iter=1000)
  - NaN handling: fillna(0.0) for both training data and prediction input
  - predict() uses predict_proba to get class probabilities; maps to Direction.UP/DOWN with confidence
  - Metrics dict includes accuracy, train_size, test_size from test split
  - Added scikit-learn>=1.3 to pyproject.toml dependencies
  - Added [[tool.mypy.overrides]] for sklearn.* (no py.typed marker)
  - Created tests/test_model_trainer.py with 9 tests:
    - test_returns_trained_model: verifies TrainedModel return type
    - test_model_produces_predictions: verifies Direction and confidence output
    - test_time_based_split_no_shuffle: verifies 80/20 split sizes
    - test_metrics_include_accuracy: verifies accuracy metric in range
    - test_feature_names_preserved: verifies column names propagated
    - test_handles_nan_in_training_data: NaN in features → still trains
    - test_handles_nan_in_prediction: NaN in prediction input → still predicts
    - test_uses_logistic_regression: verifies pipeline uses LogisticRegression
    - test_uses_standard_scaler: verifies pipeline uses StandardScaler
  - Verified: make test (124 passed), make typecheck (success)
- Files affected: src/smaps/model/__init__.py (new), src/smaps/model/trainer.py (new), tests/test_model_trainer.py (new), pyproject.toml, PRD.md, progress.txt
- Learnings for next cycles:
  - sklearn has no py.typed marker; need mypy overrides with ignore_missing_imports = true
  - TrainedModel.predict uses pipeline.classes_ to find the index of class 1 (UP) in predict_proba output
  - Edge case: if model only sees one class during training, classes_ may have only one entry
  - fillna(0.0) is a simple but effective strategy for NaN features in LogisticRegression
  - Time-based split uses simple iloc slicing: [:split_idx] for train, [split_idx:] for test — no shuffle
  - US-303 (persist model artifacts) is next — will use joblib to serialize TrainedModel.pipeline
---

## Iteration 23 — US-303: Persist model artifacts with versioning
- What was done:
  - Added migration_005_model_registry to migrations.py creating `model_registry` table
  - Columns: id (INTEGER PRIMARY KEY AUTOINCREMENT), ticker (TEXT), version (INTEGER), trained_at (TEXT ISO datetime), metrics_json (TEXT), artifact_path (TEXT)
  - Bumped SCHEMA_VERSION from 4 to 5 in db.py
  - Created src/smaps/model/registry.py with:
    - ModelRecord NamedTuple for registry rows
    - `save_model(conn, ticker, model, models_dir)` → saves TrainedModel via joblib, inserts registry row, auto-increments version per ticker
    - `load_latest_model(conn, ticker)` → queries registry for latest version, loads joblib file, returns (TrainedModel, ModelRecord) or None
    - `_next_version()` helper queries MAX(version) for ticker
  - Model saved to `models/<ticker>_v<N>.joblib` format as specified in AC
  - Added joblib>=1.3 to pyproject.toml dependencies
  - Added [[tool.mypy.overrides]] for joblib.* (no py.typed marker)
  - Created tests/test_model_registry.py with 14 tests in 3 classes:
    - TestSaveModel (7 tests): saves joblib file, artifact path format, returns ModelRecord, metrics stored, version auto-increments, tickers independent, creates dir if missing
    - TestLoadLatestModel (5 tests): round-trip save/load, loaded model produces predictions, loads latest version, no model returns None, missing artifact returns None
    - TestModelRegistry (2 tests): registry row persisted, trained_at is ISO datetime with timezone
  - Verified: make test (138 passed), make typecheck (success)
- Files affected: src/smaps/migrations.py, src/smaps/db.py, src/smaps/model/registry.py (new), tests/test_model_registry.py (new), pyproject.toml, PRD.md, progress.txt
- Learnings for next cycles:
  - joblib.dump/load serializes entire TrainedModel dataclass (not just the sklearn pipeline) — preserves feature_names and metrics
  - joblib has no py.typed marker; need mypy overrides with ignore_missing_imports = true
  - Version auto-increment uses MAX(version) query per ticker — each ticker has independent version sequences
  - models_dir is created with mkdir(parents=True, exist_ok=True) to handle first-time setup
  - load_latest_model returns None for both "no registry entry" and "artifact file missing" cases
  - trained_at stored as UTC ISO datetime via datetime.now(tz=datetime.timezone.utc)
  - cursor.lastrowid gives AUTOINCREMENT id after INSERT (same pattern as feature_snapshots)
  - US-304 (implement daily prediction function) is next — will use load_latest_model + build_features → PredictionResult
---

## Iteration 24 — US-304: Implement daily prediction function
- What was done:
  - Created src/smaps/model/predictor.py with `predict(conn, ticker, date) -> PredictionResult`
  - Loads latest model via load_latest_model(); raises RuntimeError if no model exists for ticker
  - Builds features via build_features(conn, ticker, date)
  - Calls model.predict(features) to get (Direction, confidence)
  - Returns PredictionResult with ticker, prediction_date, direction, confidence, model_version (f"v{record.version}")
  - Created tests/test_predictor.py with 8 tests:
    - test_returns_prediction_result: verifies PredictionResult return type
    - test_output_schema: verifies all fields (ticker, prediction_date, direction, confidence, model_version)
    - test_direction_is_up_or_down: verifies Direction enum membership
    - test_model_version_from_registry: verifies v2 when two models saved
    - test_raises_when_no_model: RuntimeError with no model
    - test_raises_for_unknown_ticker: RuntimeError for different ticker
    - test_uses_build_features: verifies build_features called with correct args via mock
    - test_with_mock_model: fully mocked model verifies output schema end-to-end
  - Verified: make test (146 passed), make typecheck (success)
- Files affected: src/smaps/model/predictor.py (new), tests/test_predictor.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - predict() takes conn as first arg — consistent with build_features and all DB-accessing functions
  - model_version string uses f"v{record.version}" to match the convention in ModelRecord (integer version)
  - np.abs() is a module function, not a RandomState method — use np.abs(rng.randn(n)) not rng.abs()
  - Fully mocked test (mock load_latest_model + build_features) is the cleanest way to verify output schema without real data
  - Tests with real TrainedModel work because features default to NaN/0.0 when no OHLCV/sentiment/fundamental data in DB
  - US-305 (persist predictions to database) is next — will need migration for predictions table
---

## Iteration 25 — US-305: Persist predictions to database
- What was done:
  - Added migration_006_predictions to migrations.py creating `predictions` table
  - Columns: id (INTEGER PRIMARY KEY AUTOINCREMENT), ticker (TEXT), prediction_date (TEXT), direction (TEXT), confidence (REAL), model_version (TEXT), feature_snapshot_id (INTEGER nullable), created_at (TEXT ISO datetime)
  - Bumped SCHEMA_VERSION from 5 to 6 in db.py
  - Added PredictionRecord NamedTuple to db.py for loaded prediction rows
  - Added `save_prediction(conn, prediction, feature_snapshot_id=None) -> int` returning row id
  - Added `load_prediction(conn, prediction_id) -> PredictionRecord | None` for retrieval by id
  - Direction stored as string value ("UP"/"DOWN") via Direction.value; loaded back via Direction(str)
  - created_at stored as UTC ISO datetime via datetime.now(tz=datetime.timezone.utc)
  - Created tests/test_prediction_persistence.py with 9 tests in 2 classes:
    - TestSavePrediction (4 tests): autoincrement id, sequential ids, with feature_snapshot_id, without feature_snapshot_id
    - TestLoadPrediction (5 tests): round-trip (core AC), nonexistent returns None, direction UP round-trip, direction DOWN round-trip, created_at is ISO datetime with timezone
  - Verified: make test (155 passed), make typecheck (success)
- Files affected: src/smaps/migrations.py, src/smaps/db.py, tests/test_prediction_persistence.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - Direction enum stored as TEXT using .value ("UP"/"DOWN"); loaded back via Direction(row_value) constructor
  - predictions table uses AUTOINCREMENT id (not composite PK) since same ticker+date could have multiple predictions from different model versions
  - feature_snapshot_id is nullable INTEGER — allows linking prediction to its feature snapshot for reproducibility
  - PredictionRecord NamedTuple follows same pattern as FeatureSnapshot for lightweight loaded rows
  - Migration pattern remains consistent: add function, add to MIGRATIONS list, bump SCHEMA_VERSION
  - Phase 3 prediction model is now fully complete (US-301 through US-305); Phase 4 (evaluator) is next
  - US-401 (match predictions to realized outcomes) is next — will need to compare predicted vs actual direction
---

## Iteration 26 — US-401: Match predictions to realized outcomes
- What was done:
  - Added EvalResult frozen dataclass to src/smaps/models.py with fields: prediction_id (int), actual_direction (Direction), is_correct (bool), evaluated_at (datetime.datetime)
  - Created src/smaps/evaluator.py with `evaluate_prediction(conn, prediction_id) -> EvalResult`
  - Compares predicted direction vs actual direction by looking at close prices:
    - Gets close on prediction_date from ohlcv_daily
    - Gets close on next trading day (skips weekends/holidays via SQL query: date > pred_date ORDER BY date ASC LIMIT 1)
    - If next_close >= pred_close → UP, else → DOWN
  - Weekend/holiday handling: uses actual ohlcv_daily rows rather than calendar math — naturally skips non-trading days
  - Max lookahead of 10 calendar days to find next trading day
  - Raises ValueError for: prediction not found, no price data on prediction date, no next-day price data
  - Created tests/test_evaluator.py with 11 tests in 3 classes:
    - TestEvaluatePrediction (6 tests): correct UP, incorrect UP, correct DOWN, incorrect DOWN, flat day counts as UP, evaluated_at has timezone
    - TestWeekendHolidayHandling (2 tests): skips weekend (Fri→Mon), skips holiday gap (multi-day gap)
    - TestErrorCases (3 tests): nonexistent prediction, no price on prediction date, no next-day price
  - Verified: make test (166 passed), make typecheck (success)
- Files affected: src/smaps/models.py, src/smaps/evaluator.py (new), tests/test_evaluator.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - Weekend/holiday handling is cleanest via SQL: `WHERE date > ? ORDER BY date ASC LIMIT 1` naturally finds next available trading day without calendar logic
  - Flat day (next_close == pred_close) counts as UP by convention (>= comparison)
  - EvalResult uses datetime.datetime (not str) for evaluated_at — keeps timezone info as native Python type
  - No migration needed for US-401 — evaluations table comes in US-403
  - evaluate_prediction takes conn + prediction_id — loads prediction from DB, queries ohlcv_daily for price comparison
  - US-402 (compute rolling accuracy metrics) is next — will aggregate EvalResult data for accuracy/precision/recall
---

## Iteration 27 — US-402: Compute rolling accuracy metrics
- What was done:
  - Added MetricsReport frozen dataclass to src/smaps/models.py with fields: ticker, window_start, window_end, accuracy, precision_up, recall_up, precision_down, recall_down, total_predictions, evaluated_predictions
  - Added to_dict() method on MetricsReport for JSON serialization (dates as ISO strings)
  - Added `compute_metrics(conn, ticker, window_days=90, as_of_date=None) -> MetricsReport` to src/smaps/evaluator.py
  - Queries predictions table within date window, evaluates each via evaluate_prediction, builds confusion matrix
  - Computes accuracy, precision, and recall per UP/DOWN class from TP/FP/FN counts
  - Gracefully skips unevaluable predictions (missing price data) — counted in total_predictions but not evaluated_predictions
  - Zero-division cases return 0.0 (no predictions, no predictions of a given class)
  - Created tests/test_metrics.py with 13 tests in 2 classes:
    - TestComputeMetrics (10 tests): returns MetricsReport, perfect accuracy, zero accuracy, mixed accuracy with exact metric verification, precision/recall asymmetry, no predictions, window filters old predictions, ticker isolation, skips unevaluable, window start/end boundaries
    - TestMetricsReportSerialization (3 tests): to_dict returns dict, JSON serializable, dates as ISO strings
  - Verified: make test (179 passed), make typecheck (success)
- Files affected: src/smaps/models.py, src/smaps/evaluator.py, tests/test_metrics.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - Confusion matrix for binary UP/DOWN: TP_up/FP_up/FN_up/TP_down with cross-class equivalences (FP_down = FN_up, FN_down = FP_up)
  - as_of_date optional parameter enables deterministic testing without relying on today's date
  - evaluate_prediction raises ValueError for missing price data; compute_metrics catches and skips gracefully
  - total_predictions vs evaluated_predictions distinction is important — some predictions may not yet have outcome data
  - MetricsReport.to_dict() converts dates to ISO strings for JSON compatibility since json.dumps can't serialize datetime.date
  - US-403 (persist evaluation results) is next — will add evaluations table and reports/ directory
---

## Iteration 28 — US-403: Persist evaluation results
- What was done:
  - Added migration_007_evaluations to migrations.py creating `evaluations` table
  - Columns: id (INTEGER PRIMARY KEY AUTOINCREMENT), prediction_id (INTEGER NOT NULL), actual_direction (TEXT NOT NULL), is_correct (INTEGER NOT NULL, 0/1), evaluated_at (TEXT NOT NULL, ISO datetime)
  - Bumped SCHEMA_VERSION from 6 to 7 in db.py
  - Added EvalRecord NamedTuple to db.py for loaded evaluation rows
  - Added `save_evaluation(conn, result) -> int` persisting EvalResult to evaluations table, returning row id
  - Added `load_evaluation(conn, eval_id) -> EvalRecord | None` for retrieval by id
  - Direction stored as TEXT ("UP"/"DOWN") via Direction.value; is_correct stored as INTEGER (0/1) via int(bool)
  - Added `save_metrics_report(report, reports_dir) -> Path` writing JSON metric reports to reports/ directory
  - Filename format: metrics_<ticker>_<window_end>.json
  - Creates reports/ directory with mkdir(parents=True, exist_ok=True)
  - Imported Path from pathlib and EvalResult from models into db.py
  - Created tests/test_evaluation_persistence.py with 13 tests in 3 classes:
    - TestSaveEvaluation (4 tests): autoincrement id, sequential ids, correct evaluation, incorrect evaluation
    - TestLoadEvaluation (5 tests): round-trip (core AC), nonexistent returns None, direction UP round-trip, direction DOWN round-trip, evaluated_at is ISO datetime
    - TestSaveMetricsReport (4 tests): creates report file with correct name, valid JSON content, creates directory if missing, overwrites existing report
  - Verified: make test (192 passed), make typecheck (success)
- Files affected: src/smaps/migrations.py, src/smaps/db.py, tests/test_evaluation_persistence.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - SQLite has no native boolean type; store as INTEGER 0/1, convert back with bool(row_value)
  - EvalRecord follows same NamedTuple pattern as PredictionRecord and FeatureSnapshot
  - save_metrics_report takes a dict (from MetricsReport.to_dict()) not the dataclass itself — keeps DB and file persistence decoupled
  - Path.write_text(json.dumps(..., indent=2)) is the simplest way to write formatted JSON files
  - Migration pattern remains consistent: add function, add to MIGRATIONS list, bump SCHEMA_VERSION
  - Phase 4 evaluator is now fully complete (US-401 through US-403); Phase 5 (self-learning loop) is next
  - US-501 (retrain trigger based on accuracy degradation) is next — will use compute_metrics to check rolling accuracy
---

## Iteration 29 — US-501: Implement retrain trigger based on accuracy degradation
- What was done:
  - Created src/smaps/retrainer.py with `should_retrain(conn, ticker, threshold=0.50, window_days=30, as_of_date=None) -> bool`
  - Uses compute_metrics() from evaluator.py to get rolling accuracy over the specified window
  - Returns True when accuracy < threshold (strict less-than; accuracy == threshold does NOT trigger)
  - Returns False when no evaluated predictions exist (no evidence of degradation)
  - Emits structured log events at appropriate levels:
    - WARNING on retrain trigger (includes ticker, accuracy, threshold, evaluated count, window_days)
    - INFO when accuracy is ok or when no evaluated predictions exist
  - Added as_of_date parameter for deterministic testing (same pattern as compute_metrics)
  - Created tests/test_retrainer.py with 10 tests in 2 classes:
    - TestShouldRetrain (7 tests): no predictions returns False, accuracy below threshold returns True, accuracy above threshold returns False, accuracy equals threshold returns False, custom threshold, window_days filters old predictions, ticker isolation
    - TestShouldRetrainLogging (3 tests): logs WARNING on trigger, logs INFO when ok, logs INFO when no predictions (uses caplog fixture)
  - Verified: make test (202 passed), make typecheck (success, 23 source files)
- Files affected: src/smaps/retrainer.py (new), tests/test_retrainer.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - should_retrain delegates entirely to compute_metrics — no DB queries of its own; keeps logic clean and DRY
  - Strict less-than comparison (accuracy < threshold) means accuracy exactly at threshold does NOT trigger retrain
  - caplog.at_level(logging.WARNING, logger="smaps.retrainer") is the clean way to capture and assert structured log events in pytest
  - retrainer.py module will also host retrain() (US-502), OOS validation gate (US-503), and rollback (US-504)
  - as_of_date parameter is essential for deterministic testing — avoids dependence on datetime.date.today()
  - US-502 (implement automated retraining pipeline) is next — will add retrain() function to retrainer.py
---

## Iteration 30 — US-502: Implement automated retraining pipeline
- What was done:
  - Added `retrain(conn, ticker, models_dir) -> ModelRecord` function to src/smaps/retrainer.py
  - Added `_get_trading_dates(conn, ticker) -> list[datetime.date]` helper to query all OHLCV dates
  - retrain() fetches all available OHLCV dates, builds features for each date, computes next-day direction labels (UP=1/DOWN=0)
  - Uses all historical data (not just recent window): every date pair becomes a training sample
  - Trains new model via train_model() with features DataFrame and labels Series
  - Saves model via save_model() which auto-increments version per ticker
  - Logs retrain_start (ticker, date range, count) and retrain_complete (version, accuracy, elapsed time)
  - Raises ValueError if fewer than 2 trading days available (need at least 1 feature/label pair)
  - New imports added: time, pandas, build_features, save_model, ModelRecord, train_model, TrainedModel
  - Created tests/test_retrain.py with 11 tests in 2 classes:
    - TestRetrain (9 tests): returns_model_record, new_model_version_created, version_increments_on_second_retrain, uses_all_historical_data (verifies 50 days → 49 samples), model_produces_predictions, raises_on_insufficient_data, raises_on_no_data, metrics_logged_in_record, ticker_isolation
    - TestRetrainLogging (2 tests): logs_retrain_start, logs_retrain_complete
  - Verified: make test (213 passed), make typecheck (success, 23 source files)
- Files affected: src/smaps/retrainer.py, tests/test_retrain.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - retrain() builds features for dates[:-1] (all but last) since each needs a next-day label; this gives N-1 samples for N trading days
  - Close-price lookup via dict comprehension from SQL query is efficient for label computation (no repeated DB queries)
  - Label convention: UP=1 (close_next >= close_today), DOWN=0 (strict less than) — matches evaluator's >= convention for flat days
  - _get_trading_dates uses DISTINCT + ORDER BY ASC to get clean sorted list from ohlcv_daily
  - time.monotonic() for elapsed measurement (same pattern as ingest.py)
  - retrain() takes conn as first arg — consistent with all DB-accessing functions in the codebase
  - US-503 (out-of-sample validation gate) is next — will add OOS validation before promoting new models
---

## Iteration 31 — US-503: Add out-of-sample validation gate
- What was done:
  - Added `validate_oos(new_model, features_df, labels, current_model=None, oos_days=30) -> tuple[bool, dict[str, float]]` to src/smaps/retrainer.py
  - Function holds out last `oos_days` samples as OOS validation set
  - New model promoted only if OOS accuracy strictly exceeds current model's OOS accuracy
  - When no current model exists, new model is always promoted
  - When samples <= oos_days, promotes by default (insufficient data for validation)
  - Gate decision logged with metrics at INFO level (oos_gate result=promote/blocked)
  - Added numpy import to retrainer.py for np.mean accuracy computation
  - Created tests/test_oos_validation.py with 14 tests in 2 classes:
    - TestValidateOos (10 tests): promotes no current model, promotes better model, blocks worse model, blocks equal accuracy, promotes insufficient data, oos_size matches oos_days, default oos_days=30, metrics with/without current model, accuracy in valid range
    - TestValidateOosLogging (4 tests): logs promote no current, logs promote better, logs blocked, logs insufficient data
  - Verified: make test (227 passed), make typecheck (success, 23 source files)
- Files affected: src/smaps/retrainer.py, tests/test_oos_validation.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - validate_oos is a standalone function (not integrated into retrain() yet) — US-504 rollback will integrate it
  - Linearly separable synthetic data (f1 > 0 → UP) with near-zero noise is a reliable way to create a "good" model for testing
  - Random labels produce a "bad" model with ~50% accuracy on structured data — useful as comparison baseline
  - Same model as both new and current guarantees equal accuracy → strict > comparison correctly blocks
  - numpy needed for np.mean — cleaner than Python builtins for array comparison
  - validate_oos takes pd.DataFrame + pd.Series (same types as train_model) — consistent interface
  - US-504 (rollback on regression) is next — will integrate validate_oos into the retrain flow
---

## Iteration 32 — US-504: Add rollback on regression
- What was done:
  - Added `retrain_with_validation(conn, ticker, models_dir, oos_days) -> ModelRecord | None` to src/smaps/retrainer.py
  - Integrates the full retrain cycle with OOS validation gate: builds features/labels, trains new model, loads current model, runs validate_oos()
  - If OOS gate passes (new model better): saves new model via save_model(), returns ModelRecord
  - If OOS gate fails (new model equal or worse): does NOT save new model, previous version remains active, returns None
  - Rollback event logged at WARNING level with reason=oos_gate_failed, new/current OOS accuracy, oos_size, elapsed time
  - Added `load_latest_model` to imports from model.registry
  - Key insight: same data → same model → equal OOS accuracy → strict > comparison → gate blocks → rollback (useful for deterministic tests)
  - Created tests/test_rollback.py with 10 tests in 2 classes:
    - TestRollback (7 tests): previous_model_remains_active_after_rollback, returns_none_on_rollback, model_version_not_incremented_on_rollback, promotes_when_no_current_model, promotes_model_record_returned, rollback_does_not_create_artifact_file, raises_on_insufficient_data
    - TestRollbackLogging (3 tests): logs_rollback_event, rollback_log_includes_metrics, logs_retrain_complete_on_promote
  - Verified: make test (237 passed), make typecheck (success, 23 source files)
- Files affected: src/smaps/retrainer.py, tests/test_rollback.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - retrain_with_validation() is a standalone function (not modifying retrain()) — preserves backward compatibility for existing tests and callers
  - Rollback mechanism is simple: just don't call save_model() — load_latest_model() continues to return the previous version
  - Equal OOS accuracy triggers rollback due to strict > comparison in validate_oos — this makes deterministic rollback testing easy (retrain on same data twice)
  - No new artifact file is created on rollback since save_model() is never called — verifiable by comparing directory contents before/after
  - The retrain_with_validation function duplicates retrain()'s data prep logic rather than calling retrain() because retrain() always saves — refactoring to share could be a future improvement
  - US-505 (feature drift detection) is next — will add KS-test based drift detection
---

## Iteration 33 — US-505: Add feature drift detection
- What was done:
  - Added `detect_drift(conn, ticker, window_days, p_threshold, as_of_date, reports_dir) -> dict` to src/smaps/retrainer.py
  - Uses scipy.stats.ks_2samp (two-sample Kolmogorov-Smirnov test) to compare training distribution vs recent window
  - Training distribution = features from all dates before the recent window; recent = last window_days trading dates
  - For each feature in FEATURE_KEYS, runs KS-test on non-NaN values from both distributions
  - Logs WARNING for each feature with p-value < p_threshold (default 0.05) including feature name, KS statistic, and p-value
  - Drift report persisted to `reports/drift_<date>.json` with full per-feature results
  - Edge cases handled: insufficient data (skip), no training dates (skip), NaN-only features (skip with NaN stats)
  - Added scipy>=1.11 to pyproject.toml dependencies
  - Added [[tool.mypy.overrides]] for scipy.* (no py.typed marker)
  - Imported FEATURE_KEYS from combined.py, json, Path, and ks_2samp into retrainer.py
  - Created tests/test_drift_detection.py with 16 tests in 2 classes:
    - TestDetectDrift (12 tests): returns_report_dict, no_drift_with_stable_distribution, drift_detected_with_shifted_distribution, ks_test_per_feature, insufficient_data_skips, no_training_dates_skips, report_persisted_to_file, report_creates_directory, as_of_date_defaults_to_today, ticker_isolation, custom_p_threshold, report_includes_metadata
    - TestDetectDriftLogging (4 tests): logs_warning_on_drift, logs_info_no_drift, logs_info_on_skip, drift_alert_includes_feature_name
  - Verified: make test (253 passed), make typecheck (success, 23 source files)
- Files affected: src/smaps/retrainer.py, tests/test_drift_detection.py (new), pyproject.toml, PRD.md, progress.txt
- Learnings for next cycles:
  - scipy.stats.ks_2samp returns (statistic, p_value) tuple — low p-value means distributions are significantly different
  - scipy has no py.typed marker; need mypy overrides with ignore_missing_imports = true
  - Reusing _get_trading_dates() from retrainer.py keeps drift detection consistent with retrain data splitting
  - dropna() is essential before KS-test — NaN values would cause the test to fail
  - Shifted distribution test (prices 100→500) reliably triggers drift detection in return-based features
  - p_threshold=1.0 is useful for testing "no drift" case since all p-values are ≤ 1.0
  - p_threshold=0.0 ensures nothing is flagged as drifted (p-values are always ≥ 0)
  - Phase 5 self-learning loop is now fully complete (US-501 through US-505); Phase 6 (orchestrator & scheduling) is next
  - US-601 (implement daily pipeline orchestrator) is next
---

## Iteration 34 — US-601: Implement daily pipeline orchestrator
- What was done:
  - Created src/smaps/pipeline.py with `run_pipeline(tickers, date, db_path, models_dir, lookback_days)` function
  - Chains 4 steps per ticker: ingest → predict → evaluate → retrain-if-needed
  - Each step wrapped in try/except so failure in one step does not prevent subsequent steps
  - For-loop over tickers provides ticker-level isolation (failure in one ticker doesn't block others)
  - Structured logging with run_id (uuid hex[:8]) via get_logger("smaps.pipeline", run_id=run_id)
  - Each step logged with timing via time.monotonic() (step=ingest/predict/evaluate/retrain ticker=... elapsed=...)
  - Pipeline start/complete events logged with ticker list and total elapsed
  - Ingest step: calls fetch_daily_bars, fetch_sentiment, fetch_fundamentals and persists via upsert functions
  - Predict step: calls predict() then save_prediction() to persist
  - Evaluate step: _evaluate_pending() queries for unevaluated predictions via LEFT JOIN, evaluates each with evaluate_prediction(), saves with save_evaluation()
  - Retrain step: calls should_retrain() → if True, calls retrain_with_validation() → reports "retrained", "rollback", or "not_needed"
  - Returns result dict with run_id, date, per-ticker step statuses, and elapsed time
  - Created tests/test_pipeline.py with 14 tests in 5 classes:
    - TestRunPipeline (3 tests): calls all steps, verifies call sequence order, runs all tickers
    - TestFailureIsolation (2 tests): error in one ticker doesn't block others, predict error doesn't block evaluate
    - TestResultStructure (4 tests): run_id present, date and elapsed present, all step keys present, empty ticker list
    - TestRetrainIntegration (3 tests): retrain triggered, rollback on OOS failure, retrain not needed
    - TestPipelineLogging (2 tests): logs start/complete, logs each step with elapsed
  - All tests use @patch decorators to mock external dependencies (collectors, predict, should_retrain)
  - Verified: make test (267 passed), make typecheck (success, 24 source files)
- Files affected: src/smaps/pipeline.py (new), tests/test_pipeline.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - run_pipeline creates its own DB connection via get_connection(db_path) — avoids passing conn through the public API
  - _evaluate_pending uses LEFT JOIN to find predictions without evaluations — efficient single query
  - Each step is independent within a ticker — ingest failure doesn't skip predict (old data may suffice)
  - uuid.uuid4().hex[:8] gives a compact 8-char hex run_id for log correlation
  - @patch stack order: outermost decorator = last function parameter (reversed)
  - Mock side_effect with call_order list is a clean way to verify step execution sequence
  - get_logger with run_id embeds the run_id in the handler format, not in the message content
  - US-602 (GitHub Actions daily schedule) is next
---

## Iteration 35 — US-602: Add GitHub Actions daily schedule
- What was done:
  - Created .github/workflows/daily.yml with cron schedule and workflow_dispatch trigger
  - Cron: `0 22 * * 1-5` — runs weekdays (Mon–Fri) at 22:00 UTC, after US market close
  - workflow_dispatch: enables manual trigger from GitHub Actions UI
  - Job steps: checkout → setup-python 3.11 → pip install -e . → python -m smaps.pipeline
  - Added concurrency group `daily-pipeline` with cancel-in-progress: false (pipeline should complete, not be cancelled)
  - Added `if __name__ == "__main__"` block to src/smaps/pipeline.py to support `python -m smaps.pipeline`
  - The __main__ block reads tickers and db_path from Settings (env-var-configurable), uses today's date
  - Modeled workflow after existing scaffold_verify.yml for consistency (same checkout, setup-python, shell patterns)
  - Verified: make test (267 passed), make typecheck (success, 24 source files)
- Files affected: .github/workflows/daily.yml (new), src/smaps/pipeline.py, PRD.md, progress.txt
- Learnings for next cycles:
  - Cron `1-5` in the day-of-week field = Monday through Friday (0=Sun, 1=Mon, ..., 5=Fri)
  - `python -m smaps.pipeline` requires either a __main__.py inside a pipeline package or an `if __name__ == "__main__"` block in pipeline.py
  - cancel-in-progress: false is correct for pipeline workflows (should not abort mid-run if another triggers)
  - The __main__ block uses Settings() for tickers/db_path, making the workflow configurable via GitHub Actions env vars (SMAPS_TICKERS, SMAPS_DB_PATH)
  - US-603 (CLI entry point with --tickers, --date, --dry-run, --help) is next — will extend the __main__ block with argparse
---

## Iteration 36 — US-603: Add CLI entry point
- What was done:
  - Added argparse-based CLI to src/smaps/pipeline.py via `main(argv)` function and `_build_parser()`
  - Supports `--tickers AAPL,MSFT` (comma-separated, whitespace stripped), `--date 2025-01-15` (YYYY-MM-DD), `--dry-run`, and `--help`
  - When no `--tickers` or `--date` given, defaults from Settings() and today's date are used
  - `--dry-run` logs all pipeline steps per ticker without executing them via `_run_dry()` helper
  - `if __name__ == "__main__"` block now calls `main()`, enabling `python -m smaps.pipeline` with full CLI support
  - `import argparse` moved to module-level to satisfy mypy type annotation on `_build_parser()` return type
  - Created tests/test_cli.py with 13 tests in 3 classes:
    - TestBuildParser (5 tests): help exits zero, tickers parsed, date parsed, dry-run flag, defaults
    - TestMain (5 tests): tickers+date args, tickers from args, defaults from settings, dry-run skips run_pipeline, whitespace stripped
    - TestDryRun (3 tests): logs all steps per ticker, logs complete, logs tickers and date
  - Verified: make test (280 passed), make typecheck (success, 24 source files)
- Files affected: src/smaps/pipeline.py, tests/test_cli.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - When a function does a local import (`from smaps.config import Settings`), mock at the source (`smaps.config.Settings`) not at the consumer (`smaps.pipeline.Settings`) — the local import creates a fresh binding each call
  - argparse return type annotation needs the module imported at module level for mypy — string forward refs like `"argparse.ArgumentParser"` don't resolve if the module isn't imported
  - `main(argv: list[str] | None = None)` with `parser.parse_args(argv)` makes the CLI testable — pass a list in tests, None in __main__ for sys.argv
  - `date.fromisoformat()` parses YYYY-MM-DD strings directly — no need for strptime format string
  - Phase 6 orchestrator & scheduling is now fully complete (US-601 through US-603); Phase 7 (reporting & dashboard) is next
  - US-701 (FastAPI endpoint for latest predictions) is next
---

## Iteration 37 — US-701: Add FastAPI endpoint for latest predictions
- What was done:
  - Added `fastapi>=0.110` and `uvicorn>=0.27` to pyproject.toml dependencies
  - Added `httpx>=0.27` to dev dependencies (needed for FastAPI TestClient)
  - Added [[tool.mypy.overrides]] for uvicorn.* (no py.typed marker; fastapi has py.typed so no override needed)
  - Added `get_latest_predictions(conn, ticker=None) -> list[PredictionRecord]` to src/smaps/db.py
    - Returns most recent prediction per ticker via MAX(id) subquery
    - Filterable by optional ticker parameter (returns single latest for that ticker)
    - Results ordered by prediction_date DESC
  - Created src/smaps/api.py with FastAPI app and `GET /predictions/latest` endpoint
    - Response: JSON array of {ticker, prediction_date, direction, confidence, model_version}
    - Filterable by `?ticker=AAPL` query parameter
    - Uses `check_same_thread=False` on sqlite3.connect for thread-pool compatibility
    - Connection opened and closed per request via `_get_conn()` helper
  - Created tests/test_api_predictions.py with 12 tests in 3 classes:
    - TestPredictionsLatest (8 tests): returns JSON array, latest per ticker, most recent AAPL, filter by ticker, unknown ticker empty, required fields, direction UP/DOWN, confidence range
    - TestPredictionsLatestEmptyDB (1 test): empty DB returns empty list
    - TestGetLatestPredictionsDB (3 tests): no predictions, latest per ticker, filter by ticker
  - Verified: make test (292 passed), make typecheck (success, 25 source files)
- Files affected: src/smaps/api.py (new), src/smaps/db.py, tests/test_api_predictions.py (new), pyproject.toml, PRD.md, progress.txt
- Learnings for next cycles:
  - FastAPI's TestClient (via Starlette) runs sync endpoints in a separate thread — SQLite connections created in main thread fail with ProgrammingError unless check_same_thread=False
  - Test fixture pattern: create file-based DB in main thread, populate it, close it; then patch _get_conn to open the same file with check_same_thread=False
  - FastAPI has py.typed marker — no mypy override needed; uvicorn does not
  - MAX(id) subquery pattern groups by ticker and picks the latest prediction (highest autoincrement id = most recent insert)
  - _get_conn() helper makes the endpoint testable by providing a single patch point for DB access
  - US-702 (performance summary endpoint) is next — will add GET /performance using compute_metrics
---

## Iteration 38 — US-702: Add performance summary endpoint
- What was done:
  - Added `GET /performance` endpoint to src/smaps/api.py
  - Endpoint queries DISTINCT tickers from predictions table, then calls compute_metrics(conn, ticker, window_days=90) for each
  - Response is a JSON array of MetricsReport.to_dict() objects — includes accuracy, precision_up, recall_up, precision_down, recall_down, window_start, window_end, total_predictions, evaluated_predictions per ticker
  - Follows same pattern as /predictions/latest: _get_conn() per request, try/finally conn.close()
  - Imported compute_metrics from smaps.evaluator into api.py
  - Created tests/test_api_performance.py with 10 tests in 3 classes:
    - TestPerformanceEndpoint (8 tests): returns JSON array, one entry per ticker, window dates included and valid ISO format, accuracy included, precision/recall fields present, AAPL correct prediction (accuracy=1.0), MSFT incorrect prediction (accuracy=0.0), total_predictions count
    - TestPerformanceEmptyDB (1 test): empty DB returns empty list
    - TestPerformanceUnevaluable (1 test): predictions without price data counted in total but not evaluated
  - All tests use recent dates (today - 7 days) to stay within the 90-day window of compute_metrics
  - Verified: make test (302 passed), make typecheck (success, 25 source files)
- Files affected: src/smaps/api.py, tests/test_api_performance.py (new), PRD.md, progress.txt
- Learnings for next cycles:
  - compute_metrics uses 90-day window from today by default — test data must use recent dates (within window) or accuracy/counts will be zero
  - DISTINCT ticker query on predictions table is simpler than getting tickers from Settings (shows actual tickers with predictions, not configured ones)
  - Same test fixture pattern as US-701: file-based DB, populate, close, patch _get_conn with check_same_thread=False
  - MetricsReport.to_dict() handles all JSON serialization including date->ISO string conversion
  - US-703 (minimal HTML dashboard) is next — will serve static HTML at /dashboard
---
